{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fashion_mnist_gan_v1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/motkeg/Msc-Project/blob/master/fashion_mnist_gan_v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "Kx-Fq7n01l56",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import (Input, Dense, Reshape, Flatten, Dropout,\n",
        "                                    BatchNormalization, Activation, ZeroPadding2D,\n",
        "                                    LeakyReLU, UpSampling2D, Conv2D)\n",
        "\n",
        "\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.datasets import fashion_mnist as data\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "\n",
        "JOB_DIR = \"./weights/gan_v1\"    \n",
        "USE_TPU = False\n",
        "EPOCHS = 10000\n",
        "BATCH= 128\n",
        "SAVE = 100\n",
        "\n",
        "\n",
        "if not os.path.exists(\"./samples/fashion_mnist_v1\"):\n",
        "    os.makedirs(\"./samples/fashion_mnist_v1\")\n",
        "    \n",
        "if not os.path.exists(JOB_DIR):\n",
        "    os.makedirs(JOB_DIR)\n",
        "    \n",
        "    \n",
        "    \n",
        "\n",
        "    \n",
        "if USE_TPU:\n",
        "  strategy = tf.contrib.tpu.TPUDistributionStrategy(\n",
        "                        tf.contrib.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'] ))         \n",
        "\n",
        "\n",
        "class DCGAN():\n",
        "    \n",
        "    def __init__(self,train=True):\n",
        "        \n",
        "        \n",
        "        self.img_shape = ( 28,  28, 1)\n",
        "        self.latent_dim = 100\n",
        "        self.trained = train ## this variable use to determine if we want to train the model \n",
        "\n",
        "        self.optimizer = Adam(0.0002, 0.5)\n",
        "        #define tensorboard\n",
        "        self.tensorboard = keras.callbacks.TensorBoard(log_dir=JOB_DIR,\n",
        "                                                        batch_size=BATCH,\n",
        "                                                        write_graph=True,\n",
        "                                                        histogram_freq=0,\n",
        "                                                        write_images=True,\n",
        "                                                        write_grads=True)\n",
        "        #self.checkpointer  = keras.callbacks.ModelCheckpoint(filepath=f'{JOB_DIR}/gan_model.best.hdf5', verbose = 1, save_best_only=True)                                                \n",
        "        # Build the discriminator\n",
        "        self.discriminator = self.build_D()\n",
        "        self.discriminator.compile(loss='binary_crossentropy',\n",
        "                                                optimizer=self.optimizer,\n",
        "                                                metrics=['accuracy'])\n",
        "        \n",
        "        # Build the generator\n",
        "        self.generator = self.build_G()\n",
        "\n",
        "        # The generator takes noise as input and generates imgs\n",
        "        z = Input(shape=(self.latent_dim,))\n",
        "        img = self.generator(z)\n",
        "\n",
        "        # For the combined model we will only train the generator\n",
        "        self.discriminator.trainable = False\n",
        "\n",
        "        # The discriminator takes generated images as input and determines validity\n",
        "        valid = self.discriminator(img)\n",
        "\n",
        "        # The combined model  (stacked generator and discriminator)\n",
        "        # Trains the generator to fool the discriminator\n",
        "        self.combined = Model(z, valid)\n",
        "        if USE_TPU:\n",
        "           self.combined = tf.contrib.tpu.keras_to_tpu_model(self.combined, strategy=strategy)\n",
        "\n",
        "        self.combined.compile(loss='binary_crossentropy', optimizer=self.optimizer)\n",
        "        self.tensorboard.set_model(self.combined)\n",
        "        \n",
        "\n",
        "\n",
        "    \n",
        "                                                                                                              \n",
        "\n",
        "\n",
        "    def __call__(self):\n",
        "        if self.trained:\n",
        "            # Load the dataset\n",
        "            (X_train, _), (_, _) = data.load_data()\n",
        "\n",
        "            # Rescale -1 to 1\n",
        "            X_train = X_train / 127.5 - 1.\n",
        "            X_train = np.expand_dims(X_train, axis=3)\n",
        "            #val_size = int(len(X_train)*0.1)\n",
        "            #X_valid  = X_train[:val_size] \n",
        "\n",
        "            # Adversarial ground truths\n",
        "            valid = np.ones((BATCH, 1))\n",
        "            fake = np.zeros((BATCH, 1))\n",
        "\n",
        "            for epoch in range(EPOCHS):\n",
        "\n",
        "                # ---------------------\n",
        "                #  Train Discriminator\n",
        "                # ---------------------\n",
        "\n",
        "                # Select a random half of images\n",
        "                idx = np.random.randint(0, X_train.shape[0], BATCH)\n",
        "                imgs = X_train[idx]\n",
        "\n",
        "                # Sample noise and generate a batch of new images\n",
        "                noise = np.random.normal(0, 1, (BATCH, self.latent_dim))\n",
        "                gen_imgs = self.generator.predict(noise)\n",
        "\n",
        "                # Train the discriminator (real classified as ones and generated as zeros)\n",
        "                self.discriminator.trainable = True   \n",
        "                d_loss_real = self.discriminator.train_on_batch(imgs, valid)\n",
        "                d_loss_fake = self.discriminator.train_on_batch(gen_imgs, fake)\n",
        "               \n",
        "                d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "\n",
        "                #tf.summary.scalar(\"d_loss\",d_loss) # add by moti\n",
        "                \n",
        "\n",
        "                # ---------------------\n",
        "                #  Train Generator\n",
        "                # ---------------------\n",
        "\n",
        "                # Train the generator (wants discriminator to mistake images as real)\n",
        "                self.discriminator.trainable = False\n",
        "                g_loss = self.combined.train_on_batch(noise, valid)\n",
        "                #tf.summary.scalar(\"g_loss\",g_loss) # add by moti\n",
        "\n",
        "                '''self.tensorboard.set_model(self.discriminator)\n",
        "                self.tensorboard.on_epoch_end(epoch,self.named_logs(self.discriminator,d_loss_real))\n",
        "                self.tensorboard.on_epoch_end(epoch,self.named_logs(self.discriminator,d_loss_fake))\n",
        "\n",
        "                self.tensorboard.set_model(self.combined)\n",
        "                self.tensorboard.on_epoch_end(epoch,self.named_logs(self.combined,g_loss))'''\n",
        "                \n",
        "               \n",
        "                # Plot the progress\n",
        "                print (\"%d/%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch,EPOCHS, d_loss[0], 100*d_loss[1], g_loss))\n",
        "                self.tensorboard.on_epoch_end(epoch,{\"loss\":g_loss , \"accuracy\":d_loss[1]})\n",
        "                self.combined.save(f'{JOB_DIR}/fashion_gan_model_v1.h5')\n",
        "                # If at save interval => save generated image samples\n",
        "                if epoch % SAVE == 0:\n",
        "                   self.save_imgs(epoch)\n",
        "                   \n",
        "                    \n",
        "                    \n",
        "                    \n",
        "\n",
        "                \n",
        "                   \n",
        "\n",
        "\n",
        "    def named_logs(self,model, logs):\n",
        "        result = {}\n",
        "        for l in zip(model.metrics_names, logs):\n",
        "            result[l[0]] = l[1]\n",
        "        return result\n",
        "\n",
        "    def build_G(self):\n",
        "        with tf.variable_scope(\"generator\", reuse=False):\n",
        "            model = Sequential()\n",
        "\n",
        "            model.add(Dense(128 * 7 * 7, activation=\"relu\", input_dim=self.latent_dim))\n",
        "            model.add(LeakyReLU(0.2))\n",
        "            model.add(Reshape((7, 7, 128)))\n",
        "            model.add(UpSampling2D(size=(2,2)))\n",
        "            model.add(Conv2D(64, kernel_size=5, padding=\"same\"))\n",
        "            model.add(LeakyReLU(0.2))\n",
        "            #model.add(BatchNormalization(momentum=0.8))\n",
        "            #model.add(Activation(\"relu\"))\n",
        "            model.add(UpSampling2D(size=(2,2)))\n",
        "            #model.add(Conv2D(1, kernel_size=5, padding=\"same\" ,activation=\"tanh\"))\n",
        "            #model.add(BatchNormalization(momentum=0.8))\n",
        "            #model.add(Activation(\"relu\"))\n",
        "            model.add(Conv2D(1, kernel_size=5, padding=\"same\",activation=\"tanh\"))\n",
        "            #model.add(Activation(\"tanh\"))\n",
        "            model.summary()\n",
        "\n",
        "            model.compile(loss='binary_crossentropy', optimizer=self.optimizer)\n",
        "            \n",
        "            noise = Input(shape=(self.latent_dim,))\n",
        "            img = model(noise)\n",
        "\n",
        "            return Model(noise, img)\n",
        "\n",
        "    def build_D(self):\n",
        "         with tf.variable_scope(\"discriminator\", reuse=False): \n",
        "\n",
        "            model = Sequential()\n",
        "\n",
        "            model.add(Conv2D(32, kernel_size=5, strides=2, input_shape=self.img_shape, padding=\"same\"))\n",
        "            model.add(LeakyReLU(0.2))\n",
        "            model.add(Dropout(0.3))\n",
        "            model.add(Conv2D(128, kernel_size=5, strides=2, padding=\"same\"))\n",
        "            #model.add(ZeroPadding2D(padding=((0,1),(0,1))))\n",
        "            #model.add(BatchNormalization(momentum=0.8))\n",
        "            model.add(LeakyReLU(alpha=0.2))\n",
        "            model.add(Dropout(0.3))\n",
        "            model.add(Flatten())\n",
        "            model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "            model.summary()\n",
        "            model.compile(loss='binary_crossentropy', optimizer=self.optimizer)\n",
        "\n",
        "            img = Input(shape=self.img_shape)\n",
        "            validity = model(img)\n",
        "\n",
        "            return Model(img, validity)\n",
        "\n",
        "\n",
        "    def save_imgs(self, epoch):\n",
        "        r, c = 5, 5\n",
        "        noise = np.random.normal(0, 1, (r * c, self.latent_dim))\n",
        "        gen_imgs = self.generator.predict(noise)\n",
        "\n",
        "        # Rescale images 0 - 1\n",
        "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
        "\n",
        "        fig, axs = plt.subplots(r, c)\n",
        "        cnt = 0\n",
        "        for i in range(r):\n",
        "            for j in range(c):\n",
        "                axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n",
        "                axs[i,j].axis('off')\n",
        "                cnt += 1\n",
        "        name = \"fashion_mnist_v1_{}.png\".format(epoch)        \n",
        "        fig.savefig(\"samples/fashion_mnist/\" + name)\n",
        "        plt.close()\n",
        "        return gen_imgs\n",
        "      \n",
        "      \n",
        "      \n",
        "model  = DCGAN() \n",
        "model()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0aPJs9bhK3Sf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Upload output to drive"
      ]
    },
    {
      "metadata": {
        "id": "v1AQMv8tKtiO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}